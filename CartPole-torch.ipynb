{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from scipy.signal import savgol_filter\n",
    "import collections\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[0]\n",
    "\n",
    "num_episodes = 200\n",
    "num_eval_episodes = 20\n",
    "eval_interval = 10\n",
    "\n",
    "num_repetitions = 20\n",
    "num_evaluation_points = num_episodes // eval_interval\n",
    "returns_over_repetitions = np.zeros((num_repetitions, num_evaluation_points))\n",
    "\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10def5a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(x):\n",
    "    \"\"\"Own variant of np.argmax with random tie breaking\"\"\"\n",
    "    try:\n",
    "        return np.random.choice(np.where(x == np.max(x))[0])\n",
    "    except:\n",
    "        return np.argmax(x)\n",
    "def smooth(y, window, poly=2):\n",
    "    \"\"\"\n",
    "    y: vector to be smoothed\n",
    "    window: size of the smoothing window\"\"\"\n",
    "    # print('Smoothing with window size: {} and y: {}'.format(window, y))\n",
    "    return savgol_filter(y, window, poly)\n",
    "\n",
    "\n",
    "def softmax(x, temp):\n",
    "    \"\"\"Computes the softmax of vector x with temperature parameter 'temp'\"\"\"\n",
    "    x = x / temp  # scale by temperature\n",
    "    z = x - max(x)  # substract max to prevent overflow of softmax\n",
    "    return np.exp(z) / np.sum(np.exp(z))  # compute softmax\n",
    "\n",
    "class LearningCurvePlot:\n",
    "\n",
    "    def __init__(self, title=None):\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        self.ax.set_xlabel(\"Timestep\")\n",
    "        self.ax.set_ylabel(\"Episode Return\")\n",
    "        if title is not None:\n",
    "            self.ax.set_title(title)\n",
    "\n",
    "    def add_curve(self, x, y, label=None):\n",
    "        \"\"\"y: vector of average reward results\n",
    "        label: string to appear as label in plot legend\"\"\"\n",
    "        if label is not None:\n",
    "            self.ax.plot(x, y, label=label)\n",
    "        else:\n",
    "            self.ax.plot(x, y)\n",
    "\n",
    "    def set_ylim(self, lower, upper):\n",
    "        self.ax.set_ylim([lower, upper])\n",
    "\n",
    "    def add_hline(self, height, label):\n",
    "        self.ax.axhline(height, ls=\"--\", c=\"k\", label=label)\n",
    "\n",
    "    def save(self, name=\"test.png\"):\n",
    "        \"\"\"name: string for filename of saved figure\"\"\"\n",
    "        self.ax.legend()\n",
    "        self.fig.savefig(name, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.dqn_model = nn.Sequential(\n",
    "            nn.Linear(num_states, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, num_actions),\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dqn_model(x)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for layer in self.dqn_model:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.xavier_uniform_(layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.policy_net = NeuralNetwork(num_states=4, num_actions=2)\n",
    "        self.target_net = NeuralNetwork(num_states=4, num_actions=2)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        # self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=learning_rate, amsgrad=True)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())  # Synchronize weights of target and policy network\n",
    "        self.memory = collections.deque([], maxlen=1000000)\n",
    "        self.batch_size = 32 # Need tuning and plot graphs for different settings\n",
    "        self.gamma = 0.95 # Need tuning and plot graphs for different settings\n",
    "        self.steps_done = 0\n",
    "\n",
    "        \n",
    "    def select_action(self, state, policy=\"egreedy\", epsilon=0.05):\n",
    "        if policy == \"egreedy\":\n",
    "            if np.random.rand() <= epsilon:\n",
    "                return torch.tensor([[self.env.action_space.sample()]], dtype=torch.long)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    action_values = self.policy_net(state)\n",
    "                    return torch.argmax(action_values)\n",
    "        elif policy == \"greedy\":\n",
    "            with torch.no_grad():\n",
    "                action_values = self.policy_net(state)\n",
    "                return torch.argmax(action_values)\n",
    "        # elif policy == \"softmax\":\n",
    "        #     with torch.no_grad():\n",
    "        #         action_values = self.policy_net.net(state)\n",
    "        #         # Probabilities of taking each action for state s\n",
    "        #         p = softmax(action_values, temp=1)\n",
    "        #         action = torch.multinomial(, 1)\n",
    "        #         return action\n",
    "\n",
    "    def get_sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "\n",
    "    def evaluate(self, max_episode_length=1000):\n",
    "        returns = []\n",
    "        for _ in range(num_eval_episodes):\n",
    "            state= self.env.reset()[0]\n",
    "            R_episode = 0\n",
    "            for _ in range(max_episode_length):\n",
    "                state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "                action = self.select_action(state, policy=\"greedy\")\n",
    "                next_state, reward, episode_done, episode_truncated, _= self.env.step(action.item())\n",
    "                R_episode += reward\n",
    "                if episode_done or episode_truncated:\n",
    "                    break\n",
    "                state = next_state\n",
    "            returns.append(R_episode)\n",
    "        mean_return = np.mean(returns)\n",
    "        return mean_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_learning():\n",
    "    agent = DQN_Agent(env, learning_rate=learning_rate)\n",
    "    reward_means = []\n",
    "    for e in range(num_episodes):\n",
    "        state = agent.env.reset()[0] # Sample initial state\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        episode_done = False # Completed the episode\n",
    "        episode_truncated = False # For example reaching the maximum number of steps\n",
    "\n",
    "        while not (episode_done or episode_truncated):\n",
    "            action = agent.select_action(state, policy=\"egreedy\").reshape(1, 1) # Sample action (e.g, epsilon-greedy)\n",
    "            action_index = action.item()\n",
    "            next_state, reward, episode_done, episode_truncated, _ = agent.env.step(action_index) # Simulate environment\n",
    "            reward = torch.tensor([reward])\n",
    "            next_state = None if episode_done else torch.tensor(next_state, dtype=torch.float32).unsqueeze(0) # If the epsidoe terminates no next state\n",
    "\n",
    "            # Store experience in buffer\n",
    "            agent.memory.append((state, action, next_state, reward))\n",
    "            state = next_state\n",
    "\n",
    "            # Sample a batch of experiences\n",
    "            if len(agent.memory) >= agent.batch_size:\n",
    "                experiences = agent.get_sample()\n",
    "                states_tuple, actions_tuple, next_states_tuple, rewards_tuple = zip(*experiences) # Unpack the batch\n",
    "                # Convert to tensors\n",
    "                states_batch = torch.cat(states_tuple)\n",
    "                actions_batch = torch.cat(actions_tuple)\n",
    "                rewards_batch = torch.cat(rewards_tuple)\n",
    "\n",
    "                # Calculate the current estimated Q-values by following the current policy\n",
    "                current_q_values = agent.policy_net(states_batch).gather(1, actions_batch)\n",
    "\n",
    "                # Calculate the target Q-values by Q-learning update rule\n",
    "                next_state_values = torch.zeros(agent.batch_size)\n",
    "                for i in range(len(next_states_tuple)):\n",
    "                    if next_states_tuple[i] is not None:\n",
    "                        with torch.no_grad(): # Speed up the computation by not tracking gradients\n",
    "                            next_state_values[i] = agent.target_net(next_states_tuple[i]).max(1)[0]               \n",
    "                target_q_values = (next_state_values * agent.gamma) + rewards_batch\n",
    "                \n",
    "                # Update current policy\n",
    "                # criterion = torch.nn.SmoothL1Loss() # Compute Huber loss <= works better \n",
    "                criterion = torch.nn.MSELoss()\n",
    "                loss = criterion(current_q_values, target_q_values.unsqueeze(1))\n",
    "                agent.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # torch.nn.utils.clip_grad_value_(agent.policy_net.parameters(), 100) # Clip gradients\n",
    "                agent.optimizer.step()\n",
    "\n",
    "                # Syncronize target and policy network to stabilize learning\n",
    "                agent.steps_done += 1\n",
    "                if agent.steps_done % 50 == 0:\n",
    "                    agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "        # Evaluate the performance every eval_interval episodes\n",
    "        if e % eval_interval == 0:\n",
    "            print(\"Episode: {}\".format(e))\n",
    "            returns = agent.evaluate()\n",
    "            print(f\"Evaluation: reward for episode {e} is {returns}\")\n",
    "            reward_means.append(returns)\n",
    "            \n",
    "    return reward_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_over_repetitions():\n",
    "    for i in tqdm.tqdm(range(num_repetitions)):\n",
    "        returns = DQN_learning()\n",
    "        returns_over_repetitions[i] = np.array(returns)\n",
    "\n",
    "    # Plotting the average performance\n",
    "    episodes = np.arange(num_evaluation_points) * eval_interval\n",
    "    average_returns = np.mean(returns_over_repetitions, axis=0)\n",
    "\n",
    "    plot = LearningCurvePlot(title=\"Average DQN Performance Over Repetitions\")\n",
    "    plot.add_curve(episodes, average_returns, label=\"Average Return\")\n",
    "    plot.save(name=\"dqn.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    average_over_repetitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Evaluation: reward for episode 0 is 9.65\n",
      "Episode: 10\n",
      "Evaluation: reward for episode 10 is 9.45\n",
      "Episode: 20\n",
      "Evaluation: reward for episode 20 is 9.25\n",
      "Episode: 30\n",
      "Evaluation: reward for episode 30 is 9.5\n",
      "Episode: 40\n",
      "Evaluation: reward for episode 40 is 9.65\n",
      "Episode: 50\n",
      "Evaluation: reward for episode 50 is 9.3\n",
      "Episode: 60\n",
      "Evaluation: reward for episode 60 is 10.25\n",
      "Episode: 70\n",
      "Evaluation: reward for episode 70 is 61.05\n",
      "Episode: 80\n",
      "Evaluation: reward for episode 80 is 213.65\n",
      "Episode: 90\n",
      "Evaluation: reward for episode 90 is 238.1\n",
      "Episode: 100\n",
      "Evaluation: reward for episode 100 is 320.95\n",
      "Episode: 110\n",
      "Evaluation: reward for episode 110 is 142.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
