{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from DeepQLearningAgent import DeepQLearningAgent\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from Helper import LearningCurvePlot, smooth\n",
    "import tensorflow as tf\n",
    "tf. config.set_visible_devices([], 'GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window = 19\n",
    "number_of_repetitions = 20\n",
    "number_of_episodes = 100\n",
    "l = number_of_episodes // 10\n",
    "result = np.zeros((number_of_repetitions, l))\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env_eval = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for rep in range(number_of_repetitions):\n",
    "    mean_rewards = []\n",
    "    DQN_agent = DeepQLearningAgent(env.observation_space.shape[0], env.action_space.n)\n",
    "    # Train the agent\n",
    "    for ep in range(number_of_episodes):\n",
    "        s = env.reset()[0]\n",
    "        s = np.reshape(s, [1, DQN_agent.n_states])\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = DQN_agent.select_action(s, policy=\"egreedy\", epsilon=0.1)\n",
    "            s_prime, r, done, _, _ = env.step(a)\n",
    "            s_prime = np.reshape(s_prime, [1, DQN_agent.n_states])\n",
    "            DQN_agent.remember(s, a, r, s_prime, done)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                s = s_prime\n",
    "            \n",
    "    DQN_agent.replay(32)\n",
    "    # Evaluate the agent\n",
    "    if ep % 10 == 0:\n",
    "        print(\"Episode: {}\".format(ep))\n",
    "        mean_reward = DQN_agent.evaluate(env_eval)\n",
    "        mean_rewards.append(mean_reward)\n",
    "        print(f\"Reward for episode {ep} is {mean_reward}\")\n",
    "    result[rep] = np.array(mean_rewards)\n",
    "    # learning_curve = np.mean(np.array(result),axis=0)\n",
    "    # if smoothing_window is not None: \n",
    "    #     learning_curve = smooth(learning_curve,smoothing_window)\n",
    "    # DQN_agent.save_model(f\"models/DQN_agent_{rep}.h5\")\n",
    "    # print(f\"Model {rep} trained and saved\")\n",
    "\n",
    "# Plotting the average performance\n",
    "smoothed_result = smooth(np.mean(result, axis=0), smoothing_window)\n",
    "ks = np.arange(l) * 10\n",
    "# avs = np.mean(result, axis=0)\n",
    "maxs = np.max(result, axis=0)\n",
    "mins = np.min(result, axis=0)\n",
    "\n",
    "plt.fill_between(ks, mins, maxs, alpha=0.1)\n",
    "plt.plot(ks, smoothed_result, '-o', markersize=1)\n",
    "\n",
    "plt.xlabel('Episode', fontsize=15)\n",
    "plt.ylabel('Avg. Return', fontsize=15)\n",
    "plt.show()\n",
    "plt.savefig('plot-replay-r5-ep100-w19.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
